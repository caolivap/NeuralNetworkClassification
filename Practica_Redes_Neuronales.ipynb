{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practica Redes Neuronales.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM+LVtAvEDcAamX+SaMl/aP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caolivap/NeuralNetworkClassification/blob/master/Practica_Redes_Neuronales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLRuTA7_HQ8-",
        "colab_type": "text"
      },
      "source": [
        "# Práctica para predecir imágenes de ropa\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAHAJxVEFwoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJNGA0qmH-QA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83c50cfb-edd2-4153-eb57-b4dac02fe404"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# TensorFlow y tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Librerias de ayuda\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DLdCchMdCaWQ"
      },
      "source": [
        "Usamos el dataset de datos de [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)\n",
        "que contiene mas de 70,000 imagenes en 10 categorías. Las imágenes muestran artículos individuales de ropa a una resolución baja (28 por 28 pixeles) como se ve acá:\n",
        "\n",
        "<table>\n",
        "  <tr><td>\n",
        "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
        "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
        "  </td></tr>\n",
        "  <tr><td align=\"center\">\n",
        "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
        "  </td></tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "Se usarán 60,000 imagenes para entrenar la red neuronal y 10,000 imagenes para evaluar qué tan exacto aprende la red a clasificar imágenes. Accedemos al MNIST directamente desde TensorFlow. Lo importamos y cargamos el set de datos así:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7MqDQO0KCaWS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "5c9166fd-cf27-4e8b-a831-51de25044e45"
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ3CYJCWN__K",
        "colab_type": "text"
      },
      "source": [
        "Las imagenes son 28x28 arreglos de NumPy, con valores de pixel que varían de 0 a 255. Los *labels* son un arreglo de enteros, que van del 0 al 9. Estos corresponden a la clase de ropa que la imagen representa.\n",
        "\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Label</th>\n",
        "    <th>Class</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>T-shirt/top</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Trouser</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>2</td>\n",
        "    <td>Pullover</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>3</td>\n",
        "    <td>Dress</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>4</td>\n",
        "    <td>Coat</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>5</td>\n",
        "    <td>Sandal</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>6</td>\n",
        "    <td>Shirt</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>7</td>\n",
        "    <td>Sneaker</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>8</td>\n",
        "    <td>Bag</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>9</td>\n",
        "    <td>Ankle boot</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "Cada imagen es mapeada a una única etiqueta. Ya que los *Class names* no están incluidos en el dataset, los almacenamos para usarlos luego cuando se visualicen las imágenes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA_52j7LOa7y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b726bf45-e6ca-4aeb-c7f4-1ff017d7cd4f"
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "print (class_names)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zW5k_xz1CaWX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43cb4c5c-63a9-4b82-bbf2-0a0f8f98bde2"
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TRFYHB2mCaWb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48fedcd2-3669-402d-abe4-c8ca6a79367d"
      },
      "source": [
        "len(train_labels)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XKnCTHz4CaWg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f79ddac4-5cd3-417b-fd86-58b352c01bc7"
      },
      "source": [
        "train_labels"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2KFnYlcwCaWl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4907d12-f253-4fdd-ce71-f5da869d3a38"
      },
      "source": [
        "test_images.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iJmPr5-ACaWn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c77017d3-ab6e-4e93-831e-ba3b4a99fd5d"
      },
      "source": [
        "len(test_labels)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ES6uQoLKCaWr"
      },
      "source": [
        "## Pre-procesamos el set de datos\n",
        "\n",
        "El set de datos debe ser pre-procesado antes de entrenar a la red. Podemos inspeccionar la primera imagen en el set de entrenamiento, encontraremos que los valores de los pixeles estan entre 0 y 255:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m4VEw8Ud9Quh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "b5abbf4e-a2aa-4ebd-d334-64a79456cac2"
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_images[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bW5WzIPlCaWv",
        "colab": {}
      },
      "source": [
        "train_images = train_images / 255.0\n",
        "\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oZTImqg_CaW1",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "59veuiEZCaW4"
      },
      "source": [
        "## Construir el Modelo\n",
        "\n",
        "Construir la red neuronal requiere configurar las capas del modelo y luego compilar el modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gxg1XGm0eOBy"
      },
      "source": [
        "### Configurar las Capas\n",
        "\n",
        "Los bloques de construcción básicos de una red neuronal son las *capas* o *layers*. Las capas extraen representaciones del set de datos que les alimentan. Con suerte, estas representaciones son considerables para el problema que estamos solucionando.\n",
        "\n",
        "La mayoria de aprendizaje profundo consiste de unir capas sencillas. \n",
        "La mayoria de las capas como `tf.keras.layers.Dense`, tienen parametros que son aprendidos durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9ODch-OFCaW4",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gut8A_7rCaW6"
      },
      "source": [
        "### Compilamos el modelo\n",
        "\n",
        "Antes de que el modelo esté listo para entrenar , se necesitan algunas configuraciones más. Éstas son agregadas durante el paso de compilación del modelo:\n",
        "\n",
        "* *Loss function* — Mide qué tan exacto es el modelo durante el entrenamiento. Se requiere minimizar esta función para dirigir el modelo en la dirección adecuada.\n",
        "* *Optimizer* — Representa cómo el modelo se actualiza basado en el set de datos que ve y la funcion de pérdida.\n",
        "* *Metrics* — Se usan para monitorear los pasos de entrenamiento y de pruebas.\n",
        "El siguiente ejemplo usa *accuracy* (exactitud) , la fraccion de las imágenes que son correctamente clasificadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lhan11blCaW7",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qKF6uW-BCaW-"
      },
      "source": [
        "## Entrenar el Modelo\n",
        "\n",
        "Entrenar el modelo de red neuronal requiere de los siguientes pasos:\n",
        "\n",
        "1. Entregar los datos de entrenamiento al modelo. En esta práctica, el set de datos de entrenamiento están en los arreglos `train_images` y `train_labels`.\n",
        "2. El modelo aprende a asociar imágenes y etiquetas.\n",
        "3. Le preguntamos al modelo que haga predicciones sobre un set de datos que se encuentran en el ejemplo, incluido en el arreglo `test_images`. Luego verificamos que las predicciones sean iguales a las etiquetas del arreglo `test_labels`.\n",
        "\n",
        "Para comenzar a entrenar, Hacemos uso del método `model.fit`, es llamado así porque *fit* (ajusta) el modelo al set de datos de entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xvwvpA64CaW_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8baa9a79-3291-4337-b794-1b096cebe4f6"
      },
      "source": [
        "model.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5022 - accuracy: 0.8242\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3772 - accuracy: 0.8644\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3394 - accuracy: 0.8766\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3145 - accuracy: 0.8837\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2946 - accuracy: 0.8911\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2805 - accuracy: 0.8956\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2664 - accuracy: 0.9018\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2558 - accuracy: 0.9058\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2438 - accuracy: 0.9086\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2364 - accuracy: 0.9120\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f161373fcc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W3ZVOhugCaXA"
      },
      "source": [
        "A medida que el modelo entrena, la pérdida y la exactitud son desplegadas. ¿Qué exactitud alncazó el modelo sobre el set de datos de entrenamiento?"
      ]
    }
  ]
}